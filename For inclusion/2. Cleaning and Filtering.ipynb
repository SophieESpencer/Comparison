{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literacy Outcomes of Native- and Foreign-born children - Cleaning and Filtering the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been concerned with the Syrian Civil War and the ensuing refugee crisis, since the inception of the war in 2011 when I was a student of Arabic at University of Edinburgh. Since then, I have worked directly with displaced people from this and other conflicts in Egypt, Lebanon and Greece. Whilst working with unaccompanied asylum-seeking children in Greece, I observed first hand the country's struggles to integrate asylum-seeking children into its national school system, and ultimately focused on this for my [Master's dissertation](https://sophieespencer.wordpress.com/2019/09/02/growth-unlocked/). In this dissertation, I identified the specific learning needs of refugee and asylum-seeking children as psycho-social support, protection from discrimination and bullying, and language acquisiton support. I also reviewed how other countries with high numbers of asylum-seeking children were addressing this challenge. \n",
    "\n",
    "Subsequently, I wanted to undertake a comparative investigation of countries with high numbers of asylum-seeking children to understand which countries were most successful at integrating foreign-born children into their national school systems. In the following analysis, I use data from the IEA’s Progress in International Reading Literacy Study 2021 (PIRLS 2021), an international assessment of fourth-grade students’ reading abilities conducted across 57 countries, to assess literacy outcomes for native and foreign-born children across nine countries with significant refugee populations. \n",
    "\n",
    "The goal of this study is to serve as a preliminary quantitative investigation to identify countries that demonstrate best practice in integrating foreign-born children into host-country school systems, so that further targeted investigations into successful policies and practices can be made. \n",
    "\n",
    "I wish to investigate the following research questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study is guided by the following research questions:\n",
    "\n",
    "- How do average literacy scores compare across countries with significant refugee populations?\n",
    "\n",
    "- Are there significant differences in literacy scores between native- and foreign-born children in these countries?\n",
    "\n",
    "- How does the age at which foreign-born children arrive in a host country affect their literacy outcomes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE: IEA’s Progress in International Reading Literacy Study – PIRLS 2021 Copyright © 2023 International Association for the Evaluation of Educational Achievement (IEA). Publisher: TIMSS & PIRLS International Study Center, Lynch School of Education and Human Development, Boston College."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Compiling, Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this analysis was downloaded  [here](https://pirls2021.org/data/) as separate SPSS files categorized by questionnaire type and country.\n",
    "\n",
    "After an initial review of the available data files, I selected specific files, converted them to Excel format, and concatenated them into a single dataset. \n",
    "\n",
    "To view the concatenation process, please <a href=\"https://github.com/SophieESpencer/Comparison/blob/main/Final%20Notebooks/1.%20Importing%2C%20Organising%20and%20Merging%20Datasets.ipynb\">click here</a>.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing the Data and relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)  # Optional: Set max column width\n",
    "pd.set_option('display.expand_frame_repr', False)  # Optional: Do not wrap line\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was merged and organised from multiple datafiles, including Home and Student Context Data for Turkey, Austria, Germany, Egypt, France, Iran, Jordan, The Netherlands and Sweden. To view this process, please click here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data99.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf37edb2e83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data99.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data99.xlsx'"
     ]
    }
   ],
   "source": [
    "df= pd.read_excel('data99.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook the columns were organised into Home Context Data (columns beginning with 'ASBH'), Student Context Demographic Info and  Experience in School columns (beginning with 'ASBG') and assessment score columns (beginning with 'ASR'). However, it may be more helpful to group these columns in a different way. As a first step I want to take a look at the dataframe on a macro level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!We can see from the above that many columns have object values signifying they may contain both numerical and string values - this is something I will need to fix later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Immigration Status Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important column for this analysis is 'ASBH02A', 'Was your child born in (Country)? I therefore want to firstly remove any rows of data that have no answer to this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ASBH02A'].notna() & (df['ASBH02A'] != 9)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that removing the rows that do not contain immigration data has reduced our dataset by 9002 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, this column has 'Object' dtype so I want to examine the column values a litte more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBH02A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this column contains both integers and strings. For readibility in the analysis stage I want to replace these values with ones that represent their actual meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "update_dict_ASBH02A = {1.0: 'native_born',2.0:'foreign_born','Yes': 'native_born','No':'foreign_born' }\n",
    "\n",
    "# Updating the column using map\n",
    "df['ASBH02A'] = df['ASBH02A'].map(update_dict_ASBH02A).fillna(df['ASBH02A'])\n",
    "df['ASBH02A'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Country Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fundamental column in the subsequent analysis is the 'IDCNTRY' column, which currently contains numbers representing different countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first rename the column for clarity and then update the column values from integers to more readable strings for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.rename(columns={'IDCNTRY': 'Country'})\n",
    "\n",
    "# Mapping dictionary\n",
    "update_dict = {40: 'Austria', 818: 'Egypt', 250:'France', 276:'Germany',364:'Iran',400:'Jordan',528:'Netherlands',752:'Sweden',792:'Turkey'}\n",
    "\n",
    "# Updating the column using map\n",
    "df['Country'] = df['Country'].map(update_dict).fillna(df['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Country\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Age of Arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a follow up question to the nativity question (ASBH02A) 'If No, How old was your child when he/she came to 'country'?' (ASBH02B). This column will be helpful to see how age of arrival affects foreign-born children's experience of school. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASBH02B'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the column contains both strings and integers as countries have inputted their data in different ways. However, we can see from the codebook the corresponding values. So I will assign the corresponding ordinal numbers to the string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to map patterns to integers\n",
    "def map_using_patterns(text):\n",
    "    # Dictionary of regex patterns to integers\n",
    "    pattern_to_integer = {\n",
    "        r'Younger': 1,\n",
    "        r'5': 2,\n",
    "        r'7': 3,\n",
    "        r'8': 4\n",
    "    }\n",
    "\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        for pattern, integer in pattern_to_integer.items():\n",
    "            if pd.Series([text]).str.contains(pattern).any():\n",
    "                return integer\n",
    "    return text  # Return the original value if it's not a string or no pattern matches\n",
    "\n",
    "# Apply the function to the 'ASBH02B' column\n",
    "df['ASBH02B'] = df['ASBH02B'].apply(map_using_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASBH02B'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assign more intuitive integer values to this column for clarity: -1 for native-born children, 0 for those younger than 3 years old, 3 for ages 3 to 5, 6 for ages 6 to 7, and 8 for those 8 years or older at the time of arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_column(row):\n",
    "    if row['ASBH02A'] == 'native_born':\n",
    "        return -1\n",
    "    if row['ASBH02B'] == 1:\n",
    "        return 0\n",
    "    elif row['ASBH02B'] == 2:\n",
    "        return 3\n",
    "    elif row['ASBH02B'] == 3:\n",
    "        return 6\n",
    "    elif row['ASBH02B'] == 4:\n",
    "        return 8\n",
    "    else:\n",
    "        return None  # Or any default value\n",
    "\n",
    "# Apply the function to each row\n",
    "df['MINAGEARRIVAL'] = df.apply(populate_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MINAGEARRIVAL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASDAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to round the 'ASDAGE' column as it is in decimals and I want it to be in integers for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'] = df['ASDAGE'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE']= df['ASDAGE'].replace(99.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have made the 'Country', 'ASBH02A', and 'ASBH02B' columns more readible, and removed rows with no value for the ASBH02A column, it would be helpful to understand the extent of missing data across countries and nativity status to see if we have any major gaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot_table_test = df.pivot_table(\n",
    "    index=['Country','ASBH02A'],\n",
    "    aggfunc=lambda x: x.isnull().sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of elements in each group\n",
    "total_counts = df.groupby(['Country','ASBH02A']).size()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "percentage_missing = pivot_table_test.div(total_counts, axis=0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(14,12 ))\n",
    "sns.heatmap(percentage_missing, annot=False, cmap='viridis', fmt='.2f')\n",
    "\n",
    "plt.title('Percentage of Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the blocks in yellow represent no datapoints at all. This makes sense for native-born children in the columns ASBH02B as that corresponds to age of arrival in country. Austria, Egypt, France, Jordan, Netherlands and Sweden all appear to have some missing data for in the demographic columns sourced from the Home Context Data. There is also some missing data for the experience in school columns. There is no missing data for assessment scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBH02B\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_native= df[df['ASBH02A']=='native_born']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_native[\"ASBH02B\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check germany, iran and turkey have 6 or 9 in the asbh02b column!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uniform column values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check all the unique values for each of the columns in my data set so that they are uniform (not mixed, strings with floats) and to remove any values that are not useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding of the data, I am grouping the columns into 5 groups. These are: identifier_columns, demographic_info_columns, feelings_in_school_columns, experiences_in_school_columns, assessment_score_columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The demographic columns represent our independent variables, whereas feelings_in_school_columns, negativeexperience in school and assessment score columns all contain columns which are dependent variables, metrics for assessing quality education. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_columns = ['Country','ASBH02A']\n",
    "demographic_info_columns = ['ASBH02B', 'ASBH03A', 'ASBH04', 'ASBH15A', 'ASBH15B', 'ASBH16', 'ASBH17A', 'ASBH17B', 'ASBH18AA', 'ASBH18AB', 'ASBG01', 'ASBG03', 'ASDAGE','MINAGEARRIVAL' ]\n",
    "feelings_in_school_columns = ['ASBG10A', 'ASBG10B','ASBG10C', 'ASBG10D', 'ASBG10E', 'ASBG10F']\n",
    "experiences_in_school_columns = ['ASBG11A', 'ASBG11B', 'ASBG11C', 'ASBG11D', 'ASBG11E', 'ASBG11F', 'ASBG11G', 'ASBG11H', 'ASBG11I', 'ASBG11J']\n",
    "assessment_score_columns = ['ASRREA01', 'ASRREA02', 'ASRREA03', 'ASRREA04', 'ASRREA05', 'ASRLIT01', 'ASRLIT02', 'ASRLIT03', 'ASRLIT04', 'ASRLIT05', 'ASRINF01', 'ASRINF02', 'ASRINF03', 'ASRINF04', 'ASRINF05', 'ASRIIE01', 'ASRIIE02', 'ASRIIE03', 'ASRIIE04', 'ASRIIE05', 'ASRRSI01', 'ASRRSI02', 'ASRRSI03', 'ASRRSI04', 'ASRRSI05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience in School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_experience = df[experiences_in_school_columns].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the experiences_in_school_columns are rated on the same scale. So I want to remove all the string values and assign them their associated integer value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG11A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping_frequency = {\n",
    "    'At least once a week': 1,\n",
    "    'Once or twice a month': 2,\n",
    "    'A few times a year': 3,\n",
    "    'Never': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in experiences_in_school_columns:\n",
    "    df[column]= df[column].map(ordinal_mapping_frequency).fillna(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG11A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:, experiences_in_school_columns] = df.loc[:, experiences_in_school_columns].replace(9.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG11A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_experience = df[experiences_in_school_columns].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feelings in school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_feeling = df[feelings_in_school_columns].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_feeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the feelings_in_school_columns are rated on the same scale. So I want to remove all the string values and assign them their associated integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG10A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping_agreement = { \n",
    "    'Agree a lot': 1,\n",
    "    'Agree a little': 2,\n",
    "    'Disagree a little': 3,\n",
    "    'Disagree a lot': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in feelings_in_school_columns:\n",
    "    df[column]= df[column].map(ordinal_mapping_agreement).fillna(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG10A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:, feelings_in_school_columns] = df.loc[:, feelings_in_school_columns].replace(9.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ASBG10A\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_feeling = df[feelings_in_school_columns].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_feeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Other Demographic Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will leave the other demographic columns for the time being as we are primarily concerned with one column for our initial analysis - ASBH02A - born in country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filtering the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Could Identify outliers and look at their complete rows - could be fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am concerned with removing any rows of data (each representing one student) with irregular assessment score values as such values could skew the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by adding an average score column to my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avgscore'] = df[assessment_score_columns].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avgscore'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our maximum average score is 757.011431 and our minimum is 84.541739. These scores are both quite far from the mean of 465. 34. I'm wondering if some of these are anomalies and we therefore want to delete rows with values under and over a certain amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot\n",
    "sns.boxplot(x=df['avgscore'])\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Box Plot of Your Data')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "Central Box: Represents the interquartile range (IQR), containing the middle 50% of the data.\n",
    "Line inside the Box: Indicates the median of the data.\n",
    "Whiskers: Extend to the smallest and largest values within 1.5 * IQR from the lower and upper quartiles.\n",
    "Dots outside the Whiskers: Represent potential outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to consider whether filtering the data using either the zscore method or the interquartile range would make this selection more reliable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Z-score method\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate the Z-scores\n",
    "df['zscore'] = zscore(df['avgscore'])\n",
    "\n",
    "# Define a threshold (e.g., 3)\n",
    "threshold = 3\n",
    "\n",
    "# Filter the data to remove outliers\n",
    "df_zscore_filtered = df[(df['zscore'] < threshold) & (df['zscore'] > -threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: IQR method\n",
    "Q1 = df['avgscore'].quantile(0.25)\n",
    "Q3 = df['avgscore'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the data to remove outliers\n",
    "df_iqr_filtered = df[(df['avgscore'] >= lower_bound) & (df['avgscore'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_original = df['avgscore'].describe()\n",
    "summary_zscore_filtered = df_zscore_filtered['avgscore'].describe()\n",
    "summary_iqr_filtered = df_iqr_filtered['avgscore'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Data Summary:\")\n",
    "print(summary_original)\n",
    "print(\"\\nZ-score Filtered Data Summary:\")\n",
    "print(summary_zscore_filtered)\n",
    "print(\"\\nIQR Filtered Data Summary:\")\n",
    "print(summary_iqr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(df['avgscore'])\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(df_zscore_filtered['avgscore'])\n",
    "plt.title('Z-score Filtered Data')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(df_iqr_filtered['avgscore'])\n",
    "plt.title('IQR Filtered Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df['avgscore'], kde=True)\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_zscore_filtered['avgscore'], kde=True)\n",
    "plt.title('Z-score Filtered Data')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df_iqr_filtered['avgscore'], kde=True)\n",
    "plt.title('IQR Filtered Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis and Comparison\n",
    "Summary Statistics: Compare the mean, median, standard deviation, and other statistics to see how each method affects the central tendency and spread of the data.\n",
    "Visual Inspection: Use box plots to visually inspect the data distribution and outliers in each dataset. Use histograms to compare the frequency distributions.\n",
    "Impact on Analysis: Perform key analyses (e.g., regression, clustering) on both filtered datasets and compare the results to understand the impact of outlier removal.\n",
    "Documenting Findings\n",
    "Descriptive Statistics: Summarize how the descriptive statistics differ across the datasets.\n",
    "Visual Differences: Highlight any noticeable differences in the box plots and histograms.\n",
    "Analytical Impact: Discuss how outlier removal affects your main analysis, noting any significant changes in results or interpretations.\n",
    "By comparing the filtered datasets, you can determine which method of outlier removal is more appropriate for your data and analysis goals, providing a deeper understanding of the impact of outliers on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi Square Test - remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_chi_square_test(df, score_column, category_column, bins=10):\n",
    "    \"\"\"\n",
    "    Perform a Chi-Square Test of Independence on a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        score_column (str): The column name containing the numeric scores to be binned.\n",
    "        category_column (str): The column name of the categorical variable.\n",
    "        bins (int): The number of bins for discretizing the score column. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Chi-Square test statistic, p-value, degrees of freedom, expected frequencies.\n",
    "    \"\"\"\n",
    "    # Bin the score column\n",
    "    df['binned_scores'] = pd.cut(df[score_column], bins=bins)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(df[category_column], df['binned_scores'])\n",
    "\n",
    "    # Perform Chi-Square Test of Independence\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Chi-Square Test Statistic: {chi2}\")\n",
    "    print(f\"P-value: {p}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "\n",
    "    return chi2, p, dof, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_chi_square_test(df, 'avgscore', 'ASBH02A', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_chi_square_test(df_zscore_filtered, 'avgscore', 'ASBH02A', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_chi_square_test(df_iqr_filtered, 'avgscore', 'ASBH02A', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfiltered Dataset\n",
    "Chi-Square Test Statistic: 245.83 (very large value)\n",
    "P-value: \n",
    "7.55\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "7.55×10 \n",
    "−48\n",
    "  (extremely small, essentially 0)\n",
    "Interpretation: The null hypothesis is rejected with very high confidence. There is a significant association or effect detected in the unfiltered data.\n",
    "Z-Score Filtered Dataset\n",
    "Chi-Square Test Statistic: 3.94 (very small value)\n",
    "P-value: 0.915 (very high, close to 1)\n",
    "Interpretation: The null hypothesis cannot be rejected. The z-score filtering appears to have removed most of the association or effect, likely by excluding outliers or influential data points.\n",
    "IQR Filtered Dataset\n",
    "Chi-Square Test Statistic: 248.94 (very large value, even larger than unfiltered)\n",
    "P-value: \n",
    "1.67\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "1.67×10 \n",
    "−48\n",
    "  (extremely small, essentially 0)\n",
    "Interpretation: Similar to the unfiltered data, the null hypothesis is rejected with very high confidence. This suggests that the IQR filtering preserved or even amplified the detected effect.\n",
    "Comparison of Results\n",
    "Effect of Filtering:\n",
    "\n",
    "Z-score filtering greatly reduced the test statistic and increased the p-value, possibly due to the removal of extreme values, which may have carried most of the signal.\n",
    "IQR filtering retained or enhanced the strength of the signal, leading to similar or even stronger rejection of the null hypothesis compared to the unfiltered dataset.\n",
    "Significance:\n",
    "\n",
    "The unfiltered and IQR-filtered datasets indicate a strong and statistically significant result.\n",
    "The z-score filtered dataset suggests no significant result, likely due to the removal of key influential points.\n",
    "Interpretation of Filtering Methods:\n",
    "\n",
    "Z-score Filtering: This method likely excludes extreme outliers based on standard deviations, which can minimize the effect of influential data points but may also discard important variations.\n",
    "IQR Filtering: By focusing on the central 50% of the data, this method avoids extreme outliers while retaining much of the inherent variation, preserving the signal.\n",
    "Conclusion\n",
    "The unfiltered and IQR-filtered datasets suggest strong evidence against the null hypothesis.\n",
    "Z-score filtering may overly simplify the data by removing too much information, leading to an insignificant result. It’s important to consider the nature of the data and the appropriateness of the filtering technique for the given analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "So do you think I should continue my analysis with the IQR-filtered dataset? Is it normal to get such different results between the z-score filtered df and IQR-filtered?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, continuing your analysis with the IQR-filtered dataset is a reasonable choice, particularly if your goal is to balance removing extreme outliers without losing critical information about the dataset's central tendencies. Here’s why:\n",
    "\n",
    "1. The Case for the IQR-Filtered Dataset\n",
    "Preservation of Signal: The IQR filtering retains much of the variation in the data while removing extreme outliers (points outside the interquartile range). The fact that the Chi-square test statistic is nearly identical to the unfiltered dataset suggests that the key relationships in the data remain intact.\n",
    "Outlier Management: IQR filtering removes extreme points that could skew results without discarding moderate variations that might still carry useful information.\n",
    "2. Is it Normal to Get Different Results Between Z-Score and IQR Filtering?\n",
    "Yes, it’s quite common to see different results between these two methods because they handle outliers differently:\n",
    "\n",
    "Z-Score Filtering:\n",
    "\n",
    "Focuses on removing data points that deviate significantly (e.g., beyond 3 standard deviations) from the mean.\n",
    "May remove both large and small values indiscriminately, especially in datasets with non-normal distributions or heavy tails.\n",
    "Can lead to the loss of critical variations or signals if your dataset inherently has high variability or non-Gaussian characteristics.\n",
    "IQR Filtering:\n",
    "\n",
    "Targets the tails of the distribution, but only those points beyond the 1.5x IQR from the first or third quartile.\n",
    "More robust for non-Gaussian distributions or datasets with skewed distributions because it doesn't depend on the mean and standard deviation.\n",
    "The stark difference in p-values (0.915 for z-score filtering versus \n",
    "1.67\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "1.67×10 \n",
    "−48\n",
    "  for IQR filtering) likely reflects that z-score filtering removed influential data points that were driving the significant association in your analysis. If these removed points are not true \"noise\" but meaningful data, the z-score method might underrepresent the true relationships in your data.\n",
    "\n",
    "3. Considerations\n",
    "Before proceeding exclusively with the IQR-filtered dataset:\n",
    "\n",
    "Understand Your Data:\n",
    "\n",
    "Examine the distributions of the variables and determine if they are skewed, have heavy tails, or contain clusters of outliers. This will guide you in confirming whether z-score filtering was overly aggressive.\n",
    "Validate Robustness:\n",
    "\n",
    "Cross-check key findings across both filtered datasets to ensure that your conclusions aren’t overly dependent on the choice of filtering. The fact that the unfiltered and IQR-filtered datasets yield similar results is reassuring.\n",
    "Interpret Outliers:\n",
    "\n",
    "Investigate the nature of the outliers. If outliers represent valid observations (e.g., rare but meaningful phenomena), they should not be filtered out too aggressively.\n",
    "4. Recommendation\n",
    "Based on your results and standard practices:\n",
    "\n",
    "Use the IQR-filtered dataset for your main analysis, as it preserves the data's key relationships and balances outlier removal.\n",
    "Optionally, provide a sensitivity analysis with the z-score filtered dataset in your final report. This will demonstrate the robustness of your findings under different preprocessing methods.\n",
    "In summary, the differences between filtering methods highlight the importance of choosing the one most aligned with the characteristics of your data. The IQR-filtered dataset appears to strike the right balance in your case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iqr_filtered.to_excel(\"cleandataiqr17125.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iqr_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
