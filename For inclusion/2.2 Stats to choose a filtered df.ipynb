{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('cleandata2911.xlsx', index_col= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to choose a dataset from the filtered one ( z score etc. ) and then do these correlation thingies for the numeric data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avgscore_binned'] = pd.cut(df['avgscore'], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['ASBH02A'], df['avgscore_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>avgscore_binned</th>\n",
       "      <th>(83.869, 151.789]</th>\n",
       "      <th>(151.789, 219.036]</th>\n",
       "      <th>(219.036, 286.283]</th>\n",
       "      <th>(286.283, 353.53]</th>\n",
       "      <th>(353.53, 420.777]</th>\n",
       "      <th>(420.777, 488.024]</th>\n",
       "      <th>(488.024, 555.271]</th>\n",
       "      <th>(555.271, 622.517]</th>\n",
       "      <th>(622.517, 689.764]</th>\n",
       "      <th>(689.764, 757.011]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASBH02A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>182</td>\n",
       "      <td>358</td>\n",
       "      <td>540</td>\n",
       "      <td>718</td>\n",
       "      <td>629</td>\n",
       "      <td>331</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>135</td>\n",
       "      <td>855</td>\n",
       "      <td>2163</td>\n",
       "      <td>3395</td>\n",
       "      <td>5255</td>\n",
       "      <td>7834</td>\n",
       "      <td>9656</td>\n",
       "      <td>7313</td>\n",
       "      <td>1783</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "avgscore_binned  (83.869, 151.789]  (151.789, 219.036]  (219.036, 286.283]  \\\n",
       "ASBH02A                                                                      \n",
       "No                               3                  55                 182   \n",
       "Yes                            135                 855                2163   \n",
       "\n",
       "avgscore_binned  (286.283, 353.53]  (353.53, 420.777]  (420.777, 488.024]  \\\n",
       "ASBH02A                                                                     \n",
       "No                             358                540                 718   \n",
       "Yes                           3395               5255                7834   \n",
       "\n",
       "avgscore_binned  (488.024, 555.271]  (555.271, 622.517]  (622.517, 689.764]  \\\n",
       "ASBH02A                                                                       \n",
       "No                              629                 331                  70   \n",
       "Yes                            9656                7313                1783   \n",
       "\n",
       "avgscore_binned  (689.764, 757.011]  \n",
       "ASBH02A                              \n",
       "No                                4  \n",
       "Yes                              86  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramér's V: 0.07709123028408166\n"
     ]
    }
   ],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "# Create a confusion matrix\n",
    "confusion_matrix = pd.crosstab(df['ASBH02A'], df['avgscore_binned'])\n",
    "\n",
    "# Calculate Cramér's V\n",
    "cramers_v_value = cramers_v(confusion_matrix)\n",
    "print(f\"Cramér's V: {cramers_v_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cramér's V is a statistical measure used to assess the strength of association between two nominal (categorical) variables. It is derived from the Chi-square statistic and ranges from 0 to 1, where:\n",
    "\n",
    "0 indicates no association between the variables.\n",
    "1 indicates a perfect association between the variables.\n",
    "The value you provided, 0.077, is relatively close to 0, indicating a very weak association between the two variables.\n",
    "\n",
    "Here's a general interpretation of Cramér's V:\n",
    "\n",
    "0 to 0.1: Little or no association\\\n",
    "0.1 to 0.3: Weak association\\\n",
    "0.3 to 0.5: Moderate association\\\n",
    "Above 0.5: Strong association\\\n",
    "\n",
    "Since your Cramér's V is around 0.077, it suggests that the relationship between the two categorical variables in your analysis is negligible or very weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Chi-Square Test of Independence\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 245.83458534739597\n",
      "P-value: 7.553360938216542e-48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chi-Square Test Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chi-Square Test Statistic: 245.83458534739597\n",
    "The Chi-Square test statistic quantifies how much the observed frequencies in your data deviate from the expected frequencies under the assumption that the two categorical variables are independent.\n",
    "A higher Chi-Square value generally indicates a greater difference between the observed and expected frequencies, suggesting a stronger association between the variables.\n",
    "In your case, the Chi-Square statistic is quite large, indicating a noticeable deviation from the expected distribution if the variables were independent.\n",
    "2. P-value: 7.553360938216542e-48\n",
    "The P-value represents the probability of observing a Chi-Square statistic as extreme as, or more extreme than, the one calculated, under the null hypothesis (which states that the two variables are independent).\n",
    "A very small P-value (like the one here, which is effectively 0.000...0000755) indicates that the observed association is highly unlikely to have occurred by chance.\n",
    "Common thresholds for significance are 0.05, 0.01, and 0.001. Your P-value is far smaller than any of these, meaning you can reject the null hypothesis with extremely high confidence, suggesting a statistically significant association between the two variables.\n",
    "Summary\n",
    "Chi-Square Test Statistic (245.83): Indicates a substantial deviation from what would be expected under independence.\n",
    "P-value (~0): Shows that this deviation is extremely unlikely to be due to random chance, indicating a statistically significant association.\n",
    "However, the earlier Cramér's V value of 0.077 suggests that while the association is statistically significant, it is not strong. This can happen when the sample size is large, leading to statistically significant results even for weak associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfzscore = pd.read_excel('cleandatazscore2911.xlsx', index_col= 0)\n",
    "dfiqr = pd.read_excel('cleandataiqr2911.xlsx', index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_chi_square_test(df, score_column, category_column, bins=10):\n",
    "    \"\"\"\n",
    "    Perform a Chi-Square Test of Independence on a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        score_column (str): The column name containing the numeric scores to be binned.\n",
    "        category_column (str): The column name of the categorical variable.\n",
    "        bins (int): The number of bins for discretizing the score column. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Chi-Square test statistic, p-value, degrees of freedom, expected frequencies.\n",
    "    \"\"\"\n",
    "    # Bin the score column\n",
    "    df['binned_scores'] = pd.cut(df[score_column], bins=bins)\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(df[category_column], df['binned_scores'])\n",
    "\n",
    "    # Perform Chi-Square Test of Independence\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Chi-Square Test Statistic: {chi2}\")\n",
    "    print(f\"P-value: {p}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "\n",
    "    return chi2, p, dof, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 3.9381881428980847\n",
      "P-value: 0.9154334828579949\n",
      "Degrees of Freedom: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.9381881428980847,\n",
       " 0.9154334828579949,\n",
       " 9,\n",
       " array([[  1.05510535,   1.65802269,   1.65802269,   2.61264182,\n",
       "           3.36628849,   2.86385737,   3.26580227,   4.62236629,\n",
       "           4.11993517,   5.77795786],\n",
       "        [ 19.94489465,  31.34197731,  31.34197731,  49.38735818,\n",
       "          63.63371151,  54.13614263,  61.73419773,  87.37763371,\n",
       "          77.88006483, 109.22204214]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_chi_square_test(dfzscore, 'avgscore', 'ASBH02A', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 248.9372235851522\n",
      "P-value: 1.6723183136152255e-48\n",
      "Degrees of Freedom: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(248.9372235851522,\n",
       " 1.6723183136152255e-48,\n",
       " 9,\n",
       " array([[6.13825745e+01, 1.42875650e+02, 2.16590796e+02, 3.21067302e+02,\n",
       "         4.70809952e+02, 6.04506244e+02, 6.23565674e+02, 3.64932018e+02,\n",
       "         7.39253608e+01, 4.34442879e+00],\n",
       "        [8.14617426e+02, 1.89612435e+03, 2.87440920e+03, 4.26093270e+03,\n",
       "         6.24819005e+03, 8.02249376e+03, 8.27543433e+03, 4.84306798e+03,\n",
       "         9.81074639e+02, 5.76555712e+01]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_chi_square_test(dfiqr, 'avgscore', 'ASBH02A', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfiltered Dataset\n",
    "Chi-Square Test Statistic: 245.83 (very large value)\n",
    "P-value: \n",
    "7.55\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "7.55×10 \n",
    "−48\n",
    "  (extremely small, essentially 0)\n",
    "Interpretation: The null hypothesis is rejected with very high confidence. There is a significant association or effect detected in the unfiltered data.\n",
    "Z-Score Filtered Dataset\n",
    "Chi-Square Test Statistic: 3.94 (very small value)\n",
    "P-value: 0.915 (very high, close to 1)\n",
    "Interpretation: The null hypothesis cannot be rejected. The z-score filtering appears to have removed most of the association or effect, likely by excluding outliers or influential data points.\n",
    "IQR Filtered Dataset\n",
    "Chi-Square Test Statistic: 248.94 (very large value, even larger than unfiltered)\n",
    "P-value: \n",
    "1.67\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "1.67×10 \n",
    "−48\n",
    "  (extremely small, essentially 0)\n",
    "Interpretation: Similar to the unfiltered data, the null hypothesis is rejected with very high confidence. This suggests that the IQR filtering preserved or even amplified the detected effect.\n",
    "Comparison of Results\n",
    "Effect of Filtering:\n",
    "\n",
    "Z-score filtering greatly reduced the test statistic and increased the p-value, possibly due to the removal of extreme values, which may have carried most of the signal.\n",
    "IQR filtering retained or enhanced the strength of the signal, leading to similar or even stronger rejection of the null hypothesis compared to the unfiltered dataset.\n",
    "Significance:\n",
    "\n",
    "The unfiltered and IQR-filtered datasets indicate a strong and statistically significant result.\n",
    "The z-score filtered dataset suggests no significant result, likely due to the removal of key influential points.\n",
    "Interpretation of Filtering Methods:\n",
    "\n",
    "Z-score Filtering: This method likely excludes extreme outliers based on standard deviations, which can minimize the effect of influential data points but may also discard important variations.\n",
    "IQR Filtering: By focusing on the central 50% of the data, this method avoids extreme outliers while retaining much of the inherent variation, preserving the signal.\n",
    "Conclusion\n",
    "The unfiltered and IQR-filtered datasets suggest strong evidence against the null hypothesis.\n",
    "Z-score filtering may overly simplify the data by removing too much information, leading to an insignificant result. It’s important to consider the nature of the data and the appropriateness of the filtering technique for the given analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "So do you think I should continue my analysis with the IQR-filtered dataset? Is it normal to get such different results between the z-score filtered df and IQR-filtered?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, continuing your analysis with the IQR-filtered dataset is a reasonable choice, particularly if your goal is to balance removing extreme outliers without losing critical information about the dataset's central tendencies. Here’s why:\n",
    "\n",
    "1. The Case for the IQR-Filtered Dataset\n",
    "Preservation of Signal: The IQR filtering retains much of the variation in the data while removing extreme outliers (points outside the interquartile range). The fact that the Chi-square test statistic is nearly identical to the unfiltered dataset suggests that the key relationships in the data remain intact.\n",
    "Outlier Management: IQR filtering removes extreme points that could skew results without discarding moderate variations that might still carry useful information.\n",
    "2. Is it Normal to Get Different Results Between Z-Score and IQR Filtering?\n",
    "Yes, it’s quite common to see different results between these two methods because they handle outliers differently:\n",
    "\n",
    "Z-Score Filtering:\n",
    "\n",
    "Focuses on removing data points that deviate significantly (e.g., beyond 3 standard deviations) from the mean.\n",
    "May remove both large and small values indiscriminately, especially in datasets with non-normal distributions or heavy tails.\n",
    "Can lead to the loss of critical variations or signals if your dataset inherently has high variability or non-Gaussian characteristics.\n",
    "IQR Filtering:\n",
    "\n",
    "Targets the tails of the distribution, but only those points beyond the 1.5x IQR from the first or third quartile.\n",
    "More robust for non-Gaussian distributions or datasets with skewed distributions because it doesn't depend on the mean and standard deviation.\n",
    "The stark difference in p-values (0.915 for z-score filtering versus \n",
    "1.67\n",
    "×\n",
    "1\n",
    "0\n",
    "−\n",
    "48\n",
    "1.67×10 \n",
    "−48\n",
    "  for IQR filtering) likely reflects that z-score filtering removed influential data points that were driving the significant association in your analysis. If these removed points are not true \"noise\" but meaningful data, the z-score method might underrepresent the true relationships in your data.\n",
    "\n",
    "3. Considerations\n",
    "Before proceeding exclusively with the IQR-filtered dataset:\n",
    "\n",
    "Understand Your Data:\n",
    "\n",
    "Examine the distributions of the variables and determine if they are skewed, have heavy tails, or contain clusters of outliers. This will guide you in confirming whether z-score filtering was overly aggressive.\n",
    "Validate Robustness:\n",
    "\n",
    "Cross-check key findings across both filtered datasets to ensure that your conclusions aren’t overly dependent on the choice of filtering. The fact that the unfiltered and IQR-filtered datasets yield similar results is reassuring.\n",
    "Interpret Outliers:\n",
    "\n",
    "Investigate the nature of the outliers. If outliers represent valid observations (e.g., rare but meaningful phenomena), they should not be filtered out too aggressively.\n",
    "4. Recommendation\n",
    "Based on your results and standard practices:\n",
    "\n",
    "Use the IQR-filtered dataset for your main analysis, as it preserves the data's key relationships and balances outlier removal.\n",
    "Optionally, provide a sensitivity analysis with the z-score filtered dataset in your final report. This will demonstrate the robustness of your findings under different preprocessing methods.\n",
    "In summary, the differences between filtering methods highlight the importance of choosing the one most aligned with the characteristics of your data. The IQR-filtered dataset appears to strike the right balance in your case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
