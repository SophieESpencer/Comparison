{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Optional: Set max column width\n",
    "pd.set_option('display.expand_frame_repr', False)  # Optional: Do not wrap line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('data99.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I want to remove those that have no answer to the asbh02a question because this is the most important for my analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50367 entries, 0 to 50366\n",
      "Data columns (total 57 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   IDCNTRY   50367 non-null  int64  \n",
      " 1   IDSTUD    50367 non-null  int64  \n",
      " 2   ASBH02A   41503 non-null  object \n",
      " 3   ASBH02B   16269 non-null  object \n",
      " 4   ASBH03A   41179 non-null  object \n",
      " 5   ASBH04    41231 non-null  object \n",
      " 6   ASBH15A   37405 non-null  object \n",
      " 7   ASBH15B   34597 non-null  object \n",
      " 8   ASBH16    40406 non-null  object \n",
      " 9   ASBH17A   36503 non-null  object \n",
      " 10  ASBH17B   32361 non-null  object \n",
      " 11  ASBH18AA  40161 non-null  object \n",
      " 12  ASBH18AB  35131 non-null  object \n",
      " 13  ASBG01    49428 non-null  object \n",
      " 14  ASBG03    48168 non-null  object \n",
      " 15  ASDAGE    50358 non-null  float64\n",
      " 16  ASBG10A   48311 non-null  object \n",
      " 17  ASBG10B   48077 non-null  object \n",
      " 18  ASBG10C   47893 non-null  object \n",
      " 19  ASBG10D   47822 non-null  object \n",
      " 20  ASBG10E   47844 non-null  object \n",
      " 21  ASBG10F   47945 non-null  object \n",
      " 22  ASBG11A   47967 non-null  object \n",
      " 23  ASBG11B   47756 non-null  object \n",
      " 24  ASBG11C   47634 non-null  object \n",
      " 25  ASBG11D   47497 non-null  object \n",
      " 26  ASBG11E   47543 non-null  object \n",
      " 27  ASBG11F   47843 non-null  object \n",
      " 28  ASBG11G   47707 non-null  object \n",
      " 29  ASBG11H   47701 non-null  object \n",
      " 30  ASBG11I   47647 non-null  object \n",
      " 31  ASBG11J   47613 non-null  object \n",
      " 32  ASRREA01  50367 non-null  float64\n",
      " 33  ASRREA02  50367 non-null  float64\n",
      " 34  ASRREA03  50367 non-null  float64\n",
      " 35  ASRREA04  50367 non-null  float64\n",
      " 36  ASRREA05  50367 non-null  float64\n",
      " 37  ASRLIT01  50367 non-null  float64\n",
      " 38  ASRLIT02  50367 non-null  float64\n",
      " 39  ASRLIT03  50367 non-null  float64\n",
      " 40  ASRLIT04  50367 non-null  float64\n",
      " 41  ASRLIT05  50367 non-null  float64\n",
      " 42  ASRINF01  50367 non-null  float64\n",
      " 43  ASRINF02  50367 non-null  float64\n",
      " 44  ASRINF03  50367 non-null  float64\n",
      " 45  ASRINF04  50367 non-null  float64\n",
      " 46  ASRINF05  50367 non-null  float64\n",
      " 47  ASRIIE01  50367 non-null  float64\n",
      " 48  ASRIIE02  50367 non-null  float64\n",
      " 49  ASRIIE03  50367 non-null  float64\n",
      " 50  ASRIIE04  50367 non-null  float64\n",
      " 51  ASRIIE05  50367 non-null  float64\n",
      " 52  ASRRSI01  50367 non-null  float64\n",
      " 53  ASRRSI02  50367 non-null  float64\n",
      " 54  ASRRSI03  50367 non-null  float64\n",
      " 55  ASRRSI04  50367 non-null  float64\n",
      " 56  ASRRSI05  50367 non-null  float64\n",
      "dtypes: float64(26), int64(2), object(29)\n",
      "memory usage: 22.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ASBH02A'].notna() & (df['ASBH02A'] != 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41365, 57)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to change the countries so that they are easier for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= df.rename(columns={'IDCNTRY': 'Country'})\n",
    "\n",
    "# Mapping dictionary\n",
    "update_dict = {40: 'Austria', 818: 'Egypt', 250:'France', 276:'Germany',364:'Iran',400:'Jordan',528:'Netherlands',752:'Sweden',792:'Turkey'}\n",
    "\n",
    "# Updating the column using map\n",
    "df['Country'] = df['Country'].map(update_dict).fillna(df['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to round the 'ASDAGE' column as it is in percentages and I want it to be in integers for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'] = df['ASDAGE'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should replace the 99 with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE']= df['ASDAGE'].replace(99.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASDAGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values = df.apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to assign ordinal numbers to string values in the 'ASBH02B' column which signifies age of arrival, so that they are uniform as the column includes both string and float values. changing 'Younger than .... to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to map patterns to integers\n",
    "def map_using_patterns(text):\n",
    "    # Dictionary of regex patterns to integers\n",
    "    pattern_to_integer = {\n",
    "        r'Younger': 1,\n",
    "        r'5': 2,\n",
    "        r'7': 3,\n",
    "        r'8': 4\n",
    "    }\n",
    "\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        for pattern, integer in pattern_to_integer.items():\n",
    "            if pd.Series([text]).str.contains(pattern).any():\n",
    "                return integer\n",
    "    return text  # Return the original value if it's not a string or no pattern matches\n",
    "\n",
    "# Apply the function to the 'ASBH02B' column\n",
    "df['ASBH02B'] = df['ASBH02B'].apply(map_using_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASBH02B'] = df['ASBH02B'].replace({6.0: np.nan, 9.0: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ASBH02B'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def populate_column(row):\n",
    "    if row['ASBH02B'] == 1:\n",
    "        return 0\n",
    "    elif row['ASBH02B'] == 2:\n",
    "        return 3\n",
    "    elif row['ASBH02B'] == 3:\n",
    "        return 6\n",
    "    elif row['ASBH02B'] == 4:\n",
    "        return 8\n",
    "    else:\n",
    "        return None  # Or any default value\n",
    "\n",
    "# Apply the function to each row\n",
    "df['MINAGEARRIVAL'] = df.apply(populate_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['MINAGEARRIVAL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check all the unique values for each of the columns in my data set so that they are uniform (not mixed, strings with floats) and to remove any values that are not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values = df.apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am grouping the columns into 5 groups for better understanding of the data. These are: identifier_columns, demographic_info_columns, positive_feelings_in_school, negativeexperience_in_school_columns, assessment_score_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifier_columns = ['Country','ASBH02A']\n",
    "demographic_info_columns = ['ASBH02B', 'ASBH03A', 'ASBH04', 'ASBH15A', 'ASBH15B', 'ASBH16', 'ASBH17A', 'ASBH17B', 'ASBH18AA', 'ASBH18AB', 'ASBG01', 'ASBG03', 'ASDAGE','MINAGEARRIVAL' ]\n",
    "positive_feelings_in_school = ['ASBG10A', 'ASBG10B','ASBG10C', 'ASBG10D', 'ASBG10E', 'ASBG10F']\n",
    "negativeexperience_in_school_columns = ['ASBG11A', 'ASBG11B', 'ASBG11C', 'ASBG11D', 'ASBG11E', 'ASBG11F', 'ASBG11G', 'ASBG11H', 'ASBG11I', 'ASBG11J']\n",
    "assessment_score_columns = ['ASRREA01', 'ASRREA02', 'ASRREA03', 'ASRREA04', 'ASRREA05', 'ASRLIT01', 'ASRLIT02', 'ASRLIT03', 'ASRLIT04', 'ASRLIT05', 'ASRINF01', 'ASRINF02', 'ASRINF03', 'ASRINF04', 'ASRINF05', 'ASRIIE01', 'ASRIIE02', 'ASRIIE03', 'ASRIIE04', 'ASRIIE05', 'ASRRSI01', 'ASRRSI02', 'ASRRSI03', 'ASRRSI04', 'ASRRSI05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Never                    16632\n",
       "4                         6890\n",
       "A few times a year        4660\n",
       "At least once a week      4348\n",
       "Once or twice a month     2707\n",
       "1                         2126\n",
       "3                         1099\n",
       "2                         1089\n",
       "9                          224\n",
       "Name: ASBG11A, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ASBG11A\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the negativeexperience_in_school_columns are rated on the same scale. So I want to remove all the string values and assign them their associated integer value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_mapping_frequency = {\n",
    "    'At least once a week': 1,\n",
    "    'Once or twice a month': 2,\n",
    "    'A few times a year': 3,\n",
    "    'Never': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in negativeexperience_in_school_columns:\n",
    "    df[column]= df[column].map(ordinal_mapping_frequency).fillna(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    23522\n",
       "1.0     6474\n",
       "3.0     5759\n",
       "2.0     3796\n",
       "9.0      224\n",
       "Name: ASBG11A, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ASBG11A\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16632+6890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_experience = df[negativeexperience_in_school_columns].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the positive_feelings_in_school are rated on the same scale. So I want to remove all the string values and assign them their associated integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_mapping_agreement = { \n",
    "    'Agree a lot': 1,\n",
    "    'Agree a little': 2,\n",
    "    'Disagree a little': 3,\n",
    "    'Disagree a lot': 4\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping_agreement = { \n",
    "    'Agree a lot': 1,\n",
    "    'Agree a little': 2,\n",
    "    'Disagree a little': 3,\n",
    "    'Disagree a lot': 4,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '9': None  # Assuming '9' is invalid or missing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for column in positive_feelings_in_school:\n",
    "    df[column]= df[column].map(ordinal_mapping_agreement)\n",
    "\n",
    "df[\"ASBG10A\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unique() to each column\n",
    "unique_values_school_feeling = df[positive_feelings_in_school].apply(lambda col: col.unique())\n",
    "\n",
    "print(unique_values_school_feeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "update_dict2 = {1.0: 'native_born',2.0:'foreign_born' }\n",
    "\n",
    "# Updating the column using map\n",
    "df['ASBH02A'] = df['ASBH02A'].map(update_dict2).fillna(df['ASBH02A'])\n",
    "df['ASBH02A'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "update_dict_ASBH02A_strings = {'Yes': 'native_born','No':'foreign_born' }\n",
    "\n",
    "# Updating the column using map\n",
    "df['ASBH02A'] = df['ASBH02A'].map(update_dict_ASBH02A_strings).fillna(df['ASBH02A'])\n",
    "df['ASBH02A'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested in understanding the extent of missing data in my data set and where this data is present. I am going to use heatmaps to get a better idea of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform the Groupby Operation\n",
    "grouped = df.groupby('Country')\n",
    "\n",
    "pivot_missing_data = df.pivot_table(\n",
    "    index=['Country','ASBH02A'],\n",
    "    aggfunc=lambda x: x.isnull().sum(),\n",
    ")\n",
    "\n",
    "# Calculate the total number of elements in each group\n",
    "total_counts = df.groupby(['Country','ASBH02A']).size()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "percentage_missing = pivot_missing_data.div(total_counts, axis=0) * 100\n",
    "\n",
    "print(\"Pivot Table with Percentage of Missing Values:\\n\", percentage_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(14,12 ))\n",
    "sns.heatmap(percentage_missing, annot=False, cmap='viridis', fmt='.2f')\n",
    "\n",
    "plt.title('Percentage of Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "So **Iran** and **Turkey** have no experience data.\n",
    "\n",
    "**Egypt**  and **Jordan** are missing quite a few demographic columns.\n",
    "\n",
    "\"ASBH16\" is hopes for child's level of education - most parents seem to have answered this.\n",
    "\n",
    "Germany, Iran and Turkey have not answered any questions on how often lang of test spoken at home (4) parents' level of edu 15,  professions 17, level of language 18. Iran and Turkey have also not answered sex of child G01 or how often child says lang spoken at home G03.\n",
    "\n",
    "So this is helpful for our analysis. We can compare test scores for all countries. We can only compare feelings in school and experiences for certain countries - not Iran and Turkey. \\ Likewise it is important to think about what to do with additional demographic data that goes further than whether the child was born in the country. Perhaps we should first focus on this one demographic variable and look at the others in a further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ASBG10A\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iran = df[df['Country']=='Iran']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iran['ASBG10A'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
