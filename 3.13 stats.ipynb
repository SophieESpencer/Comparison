{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('cleandata109.xlsx', index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def calculate_average(df, columns, new_column_name):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the specified columns and adds a new column with the result.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    columns (list): The list of column names to calculate the mean.\n",
    "    new_column_name (str): The name of the new column to store the averages.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[columns].mean(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading = ['ASRREA01', 'ASRREA02', 'ASRREA03', 'ASRREA04', 'ASRREA05'] \n",
    "literary_purpose = ['ASRLIT01', 'ASRLIT02', 'ASRLIT03', 'ASRLIT04', 'ASRLIT05']\n",
    "informational_purpose=['ASRINF01', 'ASRINF02', 'ASRINF03', 'ASRINF04', 'ASRINF05']\n",
    "interpreting_process= ['ASRIIE01', 'ASRIIE02', 'ASRIIE03', 'ASRIIE04', 'ASRIIE05']\n",
    "straightforward_process = ['ASRRSI01', 'ASRRSI02', 'ASRRSI03', 'ASRRSI04', 'ASRRSI05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_average(df, reading, 'reading_avg')\n",
    "df = calculate_average(df, literary_purpose, 'literary_purpose_avg')\n",
    "df = calculate_average(df, informational_purpose, 'informational_purpose_avg')\n",
    "df = calculate_average(df, interpreting_process, 'interpreting_process_avg')\n",
    "df = calculate_average(df, straightforward_process, 'straightforward_process_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = ['reading_avg', 'literary_purpose_avg', 'informational_purpose_avg','interpreting_process_avg', 'straightforward_process_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avgscore'] = df[averages].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to choose a dataset from the filtered one ( z score etc. ) and then do these correlation thingies for the numeric data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_columns = ['Country','ASBH02A','avgscore']\n",
    "demographic_info_columns = ['ASBH02B', 'ASBH03A', 'ASBH04', 'ASBH15A', 'ASBH15B', 'ASBH16', 'ASBH17A', 'ASBH17B', 'ASBH18AA', 'ASBH18AB', 'ASBG01', 'ASBG03', 'ASDAGE','MINAGEARRIVAL' ]\n",
    "positive_feelings_in_school = ['ASBG10A', 'ASBG10B','ASBG10C', 'ASBG10D', 'ASBG10E', 'ASBG10F']\n",
    "negativeexperience_in_school_columns = ['ASBG11A', 'ASBG11B', 'ASBG11C', 'ASBG11D', 'ASBG11E', 'ASBG11F', 'ASBG11G', 'ASBG11H', 'ASBG11I', 'ASBG11J']\n",
    "assessment_score_columns = ['ASRREA01', 'ASRREA02', 'ASRREA03', 'ASRREA04', 'ASRREA05', 'ASRLIT01', 'ASRLIT02', 'ASRLIT03', 'ASRLIT04', 'ASRLIT05', 'ASRINF01', 'ASRINF02', 'ASRINF03', 'ASRINF04', 'ASRINF05', 'ASRIIE01', 'ASRIIE02', 'ASRIIE03', 'ASRIIE04', 'ASRIIE05', 'ASRRSI01', 'ASRRSI02', 'ASRRSI03', 'ASRRSI04', 'ASRRSI05']\n",
    "averages = ['reading_avg', 'literary_purpose_avg', 'informational_purpose_avg','interpreting_process_avg', 'straightforward_process_avg','avgscore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below is for comparing columns that are non-numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avgscore_binned'] = pd.cut(df['avgscore'], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['ASBH02A'], df['avgscore_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>avgscore_binned</th>\n",
       "      <th>(83.869, 151.789]</th>\n",
       "      <th>(151.789, 219.036]</th>\n",
       "      <th>(219.036, 286.283]</th>\n",
       "      <th>(286.283, 353.53]</th>\n",
       "      <th>(353.53, 420.777]</th>\n",
       "      <th>(420.777, 488.024]</th>\n",
       "      <th>(488.024, 555.271]</th>\n",
       "      <th>(555.271, 622.517]</th>\n",
       "      <th>(622.517, 689.764]</th>\n",
       "      <th>(689.764, 757.011]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASBH02A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>182</td>\n",
       "      <td>358</td>\n",
       "      <td>540</td>\n",
       "      <td>718</td>\n",
       "      <td>629</td>\n",
       "      <td>331</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>135</td>\n",
       "      <td>855</td>\n",
       "      <td>2163</td>\n",
       "      <td>3395</td>\n",
       "      <td>5255</td>\n",
       "      <td>7834</td>\n",
       "      <td>9656</td>\n",
       "      <td>7313</td>\n",
       "      <td>1783</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "avgscore_binned  (83.869, 151.789]  (151.789, 219.036]  (219.036, 286.283]  \\\n",
       "ASBH02A                                                                      \n",
       "No                               3                  55                 182   \n",
       "Yes                            135                 855                2163   \n",
       "\n",
       "avgscore_binned  (286.283, 353.53]  (353.53, 420.777]  (420.777, 488.024]  \\\n",
       "ASBH02A                                                                     \n",
       "No                             358                540                 718   \n",
       "Yes                           3395               5255                7834   \n",
       "\n",
       "avgscore_binned  (488.024, 555.271]  (555.271, 622.517]  (622.517, 689.764]  \\\n",
       "ASBH02A                                                                       \n",
       "No                              629                 331                  70   \n",
       "Yes                            9656                7313                1783   \n",
       "\n",
       "avgscore_binned  (689.764, 757.011]  \n",
       "ASBH02A                              \n",
       "No                                4  \n",
       "Yes                              86  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramér's V: 0.07709123028408166\n"
     ]
    }
   ],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "# Create a confusion matrix\n",
    "confusion_matrix = pd.crosstab(df['ASBH02A'], df['avgscore_binned'])\n",
    "\n",
    "# Calculate Cramér's V\n",
    "cramers_v_value = cramers_v(confusion_matrix)\n",
    "print(f\"Cramér's V: {cramers_v_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cramér's V is a statistical measure used to assess the strength of association between two nominal (categorical) variables. It is derived from the Chi-square statistic and ranges from 0 to 1, where:\n",
    "\n",
    "0 indicates no association between the variables.\n",
    "1 indicates a perfect association between the variables.\n",
    "The value you provided, 0.077, is relatively close to 0, indicating a very weak association between the two variables.\n",
    "\n",
    "Here's a general interpretation of Cramér's V:\n",
    "\n",
    "0 to 0.1: Little or no association\\\n",
    "0.1 to 0.3: Weak association\\\n",
    "0.3 to 0.5: Moderate association\\\n",
    "Above 0.5: Strong association\\\n",
    "\n",
    "Since your Cramér's V is around 0.077, it suggests that the relationship between the two categorical variables in your analysis is negligible or very weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Chi-Square Test of Independence\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic: 245.83458534739597\n",
      "P-value: 7.553360938216542e-48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chi-Square Test Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chi-Square Test Statistic: 245.83458534739597\n",
    "The Chi-Square test statistic quantifies how much the observed frequencies in your data deviate from the expected frequencies under the assumption that the two categorical variables are independent.\n",
    "A higher Chi-Square value generally indicates a greater difference between the observed and expected frequencies, suggesting a stronger association between the variables.\n",
    "In your case, the Chi-Square statistic is quite large, indicating a noticeable deviation from the expected distribution if the variables were independent.\n",
    "2. P-value: 7.553360938216542e-48\n",
    "The P-value represents the probability of observing a Chi-Square statistic as extreme as, or more extreme than, the one calculated, under the null hypothesis (which states that the two variables are independent).\n",
    "A very small P-value (like the one here, which is effectively 0.000...0000755) indicates that the observed association is highly unlikely to have occurred by chance.\n",
    "Common thresholds for significance are 0.05, 0.01, and 0.001. Your P-value is far smaller than any of these, meaning you can reject the null hypothesis with extremely high confidence, suggesting a statistically significant association between the two variables.\n",
    "Summary\n",
    "Chi-Square Test Statistic (245.83): Indicates a substantial deviation from what would be expected under independence.\n",
    "P-value (~0): Shows that this deviation is extremely unlikely to be due to random chance, indicating a statistically significant association.\n",
    "However, the earlier Cramér's V value of 0.077 suggests that while the association is statistically significant, it is not strong. This can happen when the sample size is large, leading to statistically significant results even for weak associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
